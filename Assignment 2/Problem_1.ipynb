{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca04b154-d1c1-460c-9785-57069a0f9c54",
   "metadata": {},
   "source": [
    "# Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "616c049d-dfaf-4c11-8b77-b3490b828db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aratt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Aratt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aratt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d22295-93cf-48d6-8479-878049090254",
   "metadata": {},
   "source": [
    "# Step 2: Load and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f547cb3-7e8a-4ac6-a0c0-9451ae0140e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                            message\n",
      "0      0  Go until jurong point, crazy.. Available only ...\n",
      "1      0                      Ok lar... Joking wif u oni...\n",
      "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      0  U dun say so early hor... U c already then say...\n",
      "4      0  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "\n",
    "# Keep only the relevant columns (label and message)\n",
    "df = df[['v1', 'v2']]\n",
    "\n",
    "# Rename columns for clarity\n",
    "df.columns = ['label', 'message']\n",
    "\n",
    "# Convert labels to binary (spam=1, ham=0)\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d92c832-21e1-41d5-adbf-39185cd43710",
   "metadata": {},
   "source": [
    "# Step 3: Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44de88f4-5364-4b0e-8a6c-1921b552a250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             message  \\\n",
      "0  Go until jurong point, crazy.. Available only ...   \n",
      "1                      Ok lar... Joking wif u oni...   \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3  U dun say so early hor... U c already then say...   \n",
      "4  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                           processed  \n",
      "0  [go, jurong, point, crazy, available, bugis, n...  \n",
      "1                     [ok, lar, joking, wif, u, oni]  \n",
      "2  [free, entry, wkly, comp, win, fa, cup, final,...  \n",
      "3      [u, dun, say, early, hor, u, c, already, say]  \n",
      "4  [nah, dont, think, goes, usf, lives, around, t...  \n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to all messages\n",
    "df['processed'] = df['message'].apply(preprocess_text)\n",
    "\n",
    "# Display some processed messages\n",
    "print(df[['message', 'processed']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a8191-4c58-49f6-8f63-e86e5ea88d8f",
   "metadata": {},
   "source": [
    "# Step 4: Load Pre-trained Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "990aa923-ba15-4264-960c-ad0862aa39ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained Word2Vec model\n",
    "print(\"Loading Word2Vec model...\")\n",
    "w2v_model = api.load('word2vec-google-news-300')\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d66878-9ac9-4891-9368-b8a5da2b0b9f",
   "metadata": {},
   "source": [
    "# Step 5: Convert Messages to Fixed-Length Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd3c416e-4472-44ef-a039-26c5afa2c268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (5572, 300)\n"
     ]
    }
   ],
   "source": [
    "def message_to_vector(message, model, vector_size=300):\n",
    "    # Initialize an empty vector\n",
    "    vector = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    \n",
    "    # For each word in the message\n",
    "    for word in message:\n",
    "        if word in model.key_to_index:  # Check if word is in vocabulary\n",
    "            vector += model[word]  # Add word vector\n",
    "            count += 1\n",
    "    \n",
    "    # Average the vectors\n",
    "    if count != 0:\n",
    "        vector /= count\n",
    "    \n",
    "    return vector\n",
    "\n",
    "# Convert all messages to vectors\n",
    "X = np.array([message_to_vector(msg, w2v_model) for msg in df['processed']])\n",
    "y = df['label'].values\n",
    "\n",
    "# Check the shape of our feature matrix\n",
    "print(f\"Shape of feature matrix: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57951053-8198-4600-bef4-77a2062bb51f",
   "metadata": {},
   "source": [
    "# Step 6: Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa02c0da-8edc-43a6-b915-802d4150038e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 4457\n",
      "Testing set size: 1115\n"
     ]
    }
   ],
   "source": [
    "# Split the data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a109c8-0106-49aa-812d-39eee3019d46",
   "metadata": {},
   "source": [
    "# Step 7: Train Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e225c5ba-a660-4f83-92e6-24c5262d7ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9426\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the classifier\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b61c91-581c-43f6-aa13-5850e7e4b3ea",
   "metadata": {},
   "source": [
    "# Step 8: Create Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2622470c-f1b6-417f-856b-e3b94a7a6fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_message_class(model, w2v_model, message):\n",
    "    # Preprocess the input message\n",
    "    processed_msg = preprocess_text(message)\n",
    "    \n",
    "    # Convert to vector\n",
    "    msg_vector = message_to_vector(processed_msg, w2v_model)\n",
    "    \n",
    "    # Reshape for single prediction\n",
    "    msg_vector = msg_vector.reshape(1, -1)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(msg_vector)\n",
    "    \n",
    "    # Return human-readable label\n",
    "    return \"spam\" if prediction[0] == 1 else \"ham\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b34281d-7858-49e3-b36c-83ad88fff07d",
   "metadata": {},
   "source": [
    "# Step 9: Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d15727b-55ca-491c-a5bf-69f90f21162e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: WINNER!! You've been selected for a free vacation. Call now to claim!\n",
      "Prediction: spam\n",
      "\n",
      "Message: Hey, how about meeting for coffee tomorrow?\n",
      "Prediction: ham\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "test_message = \"WINNER!! You've been selected for a free vacation. Call now to claim!\"\n",
    "prediction = predict_message_class(classifier, w2v_model, test_message)\n",
    "print(f\"Message: {test_message}\")\n",
    "print(f\"Prediction: {prediction}\")\n",
    "\n",
    "test_message2 = \"Hey, how about meeting for coffee tomorrow?\"\n",
    "prediction2 = predict_message_class(classifier, w2v_model, test_message2)\n",
    "print(f\"\\nMessage: {test_message2}\")\n",
    "print(f\"Prediction: {prediction2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99b5317-3336-4434-b7ca-3a31f4dbee41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
