{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af8790bd-b000-4b63-b855-d6c9f593d512",
   "metadata": {},
   "source": [
    "# Step 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dc36065-6a41-4119-84ab-c1af18256618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\aratt\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\aratt\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\aratt\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aratt\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\aratt\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aratt\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aratt\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aratt\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: click in c:\\users\\aratt\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\aratt\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aratt\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aratt\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\aratt\\anaconda3\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\aratt\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aratt\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aratt\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aratt\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aratt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aratt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aratt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Aratt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  airline_sentiment                                               text\n",
      "0           neutral                @VirginAmerica What @dhepburn said.\n",
      "1          positive  @VirginAmerica plus you've added commercials t...\n",
      "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
      "3          negative  @VirginAmerica it's really aggressive to blast...\n",
      "4          negative  @VirginAmerica and it's a really big bad thing...\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas nltk gensim scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load the dataset (assuming it's named 'Tweets.csv')\n",
    "df = pd.read_csv('Tweets.csv')\n",
    "print(df[['airline_sentiment', 'text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77859a9a-e2fe-4ccc-94b8-c1fb32d37bd3",
   "metadata": {},
   "source": [
    "# Step 2: Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e29d9987-09a2-45ad-813b-441b5012897f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0                @VirginAmerica What @dhepburn said.   \n",
      "1  @VirginAmerica plus you've added commercials t...   \n",
      "2  @VirginAmerica I didn't today... Must mean I n...   \n",
      "3  @VirginAmerica it's really aggressive to blast...   \n",
      "4  @VirginAmerica and it's a really big bad thing...   \n",
      "\n",
      "                                      processed_text  \n",
      "0                                               said  \n",
      "1       plus youve added commercial experience tacky  \n",
      "2       didnt today must mean need take another trip  \n",
      "3  really aggressive blast obnoxious entertainmen...  \n",
      "4                               really big bad thing  \n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove mentions (@username) and hashtags\n",
    "    text = re.sub(r'@\\w+|\\#\\w+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Expand contractions\n",
    "    contractions = {\n",
    "        \"don't\": \"do not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"i'm\": \"i am\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"ain't\": \"am not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"you've\": \"you have\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"i'd\": \"i would\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"he'd\": \"he would\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"they'll\": \"they will\"\n",
    "    }\n",
    "    for cont, expanded in contractions.items():\n",
    "        text = text.replace(cont, expanded)\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in lemmatized_tokens if word not in stop_words]\n",
    "    \n",
    "    # Join tokens back to string\n",
    "    processed_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Apply preprocessing to all tweets\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "print(df[['text', 'processed_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea487b-e8b5-4bcf-8a90-71ec853a8fcf",
   "metadata": {},
   "source": [
    "# Step 3: Load Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39687fed-a8d4-4876-8003-ae20e175fbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained Word2Vec model\n",
    "print(\"Loading Word2Vec model...\")\n",
    "w2v_model = api.load('word2vec-google-news-300')\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73d551c-82f5-4d2e-aa94-4cdec5412f09",
   "metadata": {},
   "source": [
    "# Step 4: Vectorize Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "904948a0-9450-4208-9527-06d6554061e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (14640, 300)\n"
     ]
    }
   ],
   "source": [
    "def tweet_to_vector(tweet, model, vector_size=300):\n",
    "    words = tweet.split()\n",
    "    vector = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            vector += model[word]\n",
    "            count += 1\n",
    "    \n",
    "    if count != 0:\n",
    "        vector /= count\n",
    "    \n",
    "    return vector\n",
    "\n",
    "# Vectorize all tweets\n",
    "X = np.array([tweet_to_vector(tweet, w2v_model) for tweet in df['processed_text']])\n",
    "y = df['airline_sentiment'].values\n",
    "\n",
    "print(f\"Shape of feature matrix: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf27f97-eb22-4c82-92c6-5b28c84656f9",
   "metadata": {},
   "source": [
    "# Step 5: Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bf401b4-2ed9-4a07-be3a-0638f8e95efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 11712\n",
      "Test set size: 2928\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b1cab-c8e7-492d-a4a1-95a1dd719ee4",
   "metadata": {},
   "source": [
    "# Step 6: Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fdd8a5c-b68b-4f1b-aeff-c2b937b12a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aratt\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7770\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model\n",
    "lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e69b5-7974-43f5-bc88-f3463020f088",
   "metadata": {},
   "source": [
    "# Step 7: Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db95bc00-968a-400f-99fa-e86887149282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "def predict_tweet_sentiment(model, w2v_model, tweet):\n",
    "    # Preprocess the tweet\n",
    "    processed_tweet = preprocess_text(tweet)\n",
    "    \n",
    "    # Convert to vector\n",
    "    tweet_vector = tweet_to_vector(processed_tweet, w2v_model)\n",
    "    \n",
    "    # Reshape for single prediction\n",
    "    tweet_vector = tweet_vector.reshape(1, -1)\n",
    "    \n",
    "    # Predict sentiment\n",
    "    sentiment = model.predict(tweet_vector)[0]\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "# Example usage\n",
    "sample_tweet = \"The flight was amazing and the crew was very friendly!\"\n",
    "predicted_sentiment = predict_tweet_sentiment(lr_model, w2v_model, sample_tweet)\n",
    "print(f\"Predicted sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa082b-8c75-4283-9351-2f35ad45e15f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
